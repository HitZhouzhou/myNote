# 条件概率与独立性总结
## 主线
1. 条件概率,乘法定理
2. 全概率公式
3. 贝叶斯公式
4. 试验的独立性
5. 重复独立试验,二项概率公式
6. **二项概率的泊松逼近**(新知识)
## 1.条件概率,乘法定理
### 条件概率
提出:在实际中,我们除了要考虑实际A发生的概率外,有时还要考虑在实际B发生的基础上,事件A发生的概率.  
因此提出:条件概率,记作$P(A|B) $,读作"事件A在实际B发生的条件下的条件概率".
$$P(A|B)=\frac{P(AB)}{P(B)} $$

定义:设A和B为两个任意的事件,且$P(B)>0 $,则称$P(AB)/P(B) $为事件A在实际B发生的条件下的条件概率.
$$P(A|B)=\frac{P(AB)}{P(B)} $$

定理: 条件概率$P(A|B)=\frac{P(AB)}{P(B)} $满足概率的三条公理:
1. $P(A|B)\geq 0 $
2. $P(S|B)=1 $
3. 若$A_1,A_2... $互不相容,则有$$P((A_1,A_2+\dots)|B)=P(A_1|B)+P(A_2|B)+\dots $$

因此,由这三条公理推导出的定理对条件概率也适用.

### 乘法定理
两个事件积的概率等于其中一个事件的概率与另一事件在前一事件发生条件下的条件概率的乘积,听起来很绕,其实就是:
$$P(AB)=P(A)P(B|A) $$
推广到n个事件上:$$P(A_1A_2\dots A_n)P(A_1)P(A_2|A_1)\dots P(A_n|A_1A_2\dots P_{n-1}) $$
## 全概率公式
提出:在概率的计算中,人们希望通过已知的,简单的概率去计算未知的,较为复杂的概率.

定理(全概率公式) 设$A_1,A_2,...A_n $是互不相容的的事件,且$P(A_i)>0(i=1,2,\dots n)$,若对任一事件B,都有$A_1+A_2+...+A_n\supset B $(即B发生一定导致$A_1,A_2,\dots A_n $中的一个发生),有:
$$P(B)=\sum_{i=1}^{n}P(B|A_i) $$
## 贝叶斯公式
提出:在诊病,已知出现某种症状的许多病因,假如在某次诊断中出现了某种症状,就需要研究引起这种症状的各种病因的概率是多少,哪种病因出现的概率最大.

定理:设A_1,A_2,...,A_n是互不相容的事件,且$P(A_i) $大于0.若对任意事件B都有$A_1+A_2+\dots+A_n\supset B $则$$P(A_i|B)=\frac{P(A_i)P(B|A_i)}{\sum_{j=1}^{n}P(A_j)P(B|A_j)} $$
> 式中,$P_i $为先验概率,这种概率一般在试验之前就已经知道了,$P(B|A_i) $为后验概率,它反应在事件发生后对各种可能的原因的概率的新知识.

## 事件的独立性
提出:如上可知,条件概率和无条件概率一般是不相等的,即$P(B|A)\neq P(B) $即B的发生对A的概率会有影响.如果P(B|A)=P(B),则代表A事件的发生对B事件没有影响,因此可以认为A与B的发生是相互独立的.
$$P(B|A)=\frac{P(AB)}{P(A)}=P(B) \implies P(AB)=P(A)P(B) $$  
定义:设A,B为两个任意的事件

